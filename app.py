import streamlit as st
import PyPDF2
from docx import Document
import io
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import pandas as pd
import requests
import tempfile
import os
import json

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="AHN'S AI Assistant",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ê¹”ë”í•œ í™”ì´íŠ¸ ëª¨ë“œ CSS
st.markdown("""
<style>
    /* ë©”ì¸ ì»¨í…Œì´ë„ˆ */
    .main .block-container {
        padding-top: 2rem;
        padding-bottom: 120px;
        padding-left: 2rem;
        padding-right: 2rem;
        max-width: none;
    }
    
    /* ì‚¬ì´ë“œë°” ìŠ¤íƒ€ì¼ */
    .css-1d391kg {
        background-color: #f8f9fa;
        border-right: 1px solid #e9ecef;
    }
    
    /* í—¤ë” ìŠ¤íƒ€ì¼ */
    h1 {
        color: #2c3e50;
        font-weight: 700;
        margin-bottom: 0.5rem;
    }
    
    h2, h3 {
        color: #34495e;
        font-weight: 600;
    }
    
    /* ë²„íŠ¼ ìŠ¤íƒ€ì¼ */
    .stButton > button {
        background-color: #3498db;
        color: white;
        border: none;
        border-radius: 6px;
        font-weight: 500;
        padding: 0.6rem 1.2rem;
        transition: all 0.2s ease;
        box-shadow: 0 2px 4px rgba(52, 152, 219, 0.2);
    }
    
    .stButton > button:hover {
        background-color: #2980b9;
        transform: translateY(-1px);
        box-shadow: 0 4px 8px rgba(52, 152, 219, 0.3);
    }
    
    /* Primary ë²„íŠ¼ */
    .stButton > button[kind="primary"] {
        background-color: #27ae60;
        box-shadow: 0 2px 4px rgba(39, 174, 96, 0.2);
    }
    
    .stButton > button[kind="primary"]:hover {
        background-color: #229954;
        box-shadow: 0 4px 8px rgba(39, 174, 96, 0.3);
    }
    
    /* ì…ë ¥ í•„ë“œ ìŠ¤íƒ€ì¼ */
    .stTextInput > div > div > input {
        border: 2px solid #e9ecef;
        border-radius: 6px;
        padding: 0.6rem;
        transition: border-color 0.2s ease;
    }
    
    .stTextInput > div > div > input:focus {
        border-color: #3498db;
        box-shadow: 0 0 0 3px rgba(52, 152, 219, 0.1);
    }
    
    /* íŒŒì¼ ì—…ë¡œë” ìŠ¤íƒ€ì¼ */
    .stFileUploader {
        border: 2px dashed #bdc3c7;
        border-radius: 8px;
        padding: 1.5rem;
        background-color: #f8f9fa;
        transition: all 0.2s ease;
    }
    
    .stFileUploader:hover {
        border-color: #3498db;
        background-color: #ecf0f1;
    }
    
    /* ë©”ì‹œì§€ ìŠ¤íƒ€ì¼ */
    .stSuccess {
        background-color: #d4edda;
        border: 1px solid #c3e6cb;
        color: #155724;
        border-radius: 6px;
    }
    
    .stInfo {
        background-color: #d1ecf1;
        border: 1px solid #bee5eb;
        color: #0c5460;
        border-radius: 6px;
    }
    
    .stWarning {
        background-color: #fff3cd;
        border: 1px solid #ffeaa7;
        color: #856404;
        border-radius: 6px;
    }
    
    .stError {
        background-color: #f8d7da;
        border: 1px solid #f5c6cb;
        color: #721c24;
        border-radius: 6px;
    }
    
    /* í™•ì¥ ê°€ëŠ¥í•œ ì„¹ì…˜ */
    .streamlit-expanderHeader {
        background-color: #f8f9fa;
        border: 1px solid #e9ecef;
        border-radius: 6px;
        font-weight: 500;
    }
    
    .streamlit-expanderContent {
        border: 1px solid #e9ecef;
        border-top: none;
        border-radius: 0 0 6px 6px;
        background-color: #ffffff;
    }
    
    /* ë©”íŠ¸ë¦­ ì»¨í…Œì´ë„ˆ */
    [data-testid="metric-container"] {
        background-color: #ffffff;
        border: 1px solid #e9ecef;
        border-radius: 6px;
        padding: 1rem;
        box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1);
    }
    
    /* êµ¬ë¶„ì„  */
    hr {
        border-color: #e9ecef;
        margin: 1.5rem 0;
    }
    
    /* ì±„íŒ… ì…ë ¥ì°½ ìŠ¤íƒ€ì¼ ìˆ˜ì • */
    [data-testid="stChatInput"] textarea {
        border: 2px solid #e9ecef !important;
        border-radius: 25px !important;
        padding: 12px 20px !important;
        font-size: 1rem !important;
        transition: border-color 0.2s ease !important;
        outline: none !important;
        box-shadow: none !important;
    }
    
    [data-testid="stChatInput"] textarea:focus {
        border-color: #3498db !important;
        box-shadow: 0 0 0 3px rgba(52, 152, 219, 0.1) !important;
        outline: none !important;
    }
    
    [data-testid="stChatInput"] textarea::placeholder {
        color: #7f8c8d !important;
    }
    
    /* ë¹¨ê°„ ë°‘ì¤„ ë° í…Œë‘ë¦¬ ì œê±° */
    [data-testid="stChatInput"] textarea:invalid {
        border-color: #e9ecef !important;
        box-shadow: none !important;
    }
    
    /* í¬ì»¤ìŠ¤ ì‹œ ë¹¨ê°„ í…Œë‘ë¦¬ ë°©ì§€ */
    [data-testid="stChatInput"] textarea:focus:invalid {
        border-color: #3498db !important;
        box-shadow: 0 0 0 3px rgba(52, 152, 219, 0.1) !important;
    }
    
    /* ì±„íŒ… ì…ë ¥ì°½ ìœ„ì¹˜ */
    .stChatInput {
        position: fixed;
        bottom: 0;
        left: 320px;
        right: 0;
        background: white;
        border-top: 1px solid #e9ecef;
        padding: 1rem 2rem;
        z-index: 999;
        box-shadow: 0 -2px 10px rgba(0,0,0,0.1);
    }
    
    /* ì‚¬ì´ë“œë°” ì¶•ì†Œì‹œ */
    @media (max-width: 768px) {
        .stChatInput {
            left: 0;
            padding: 1rem;
        }
        
        .main .block-container {
            padding-left: 1rem;
            padding-right: 1rem;
        }
    }
    
    /* ë§í¬ ìƒ‰ìƒ */
    a {
        color: #3498db;
        text-decoration: none;
    }
    
    a:hover {
        color: #2980b9;
        text-decoration: underline;
    }
</style>
""", unsafe_allow_html=True)

# ì œëª© ë° í—¤ë”
st.title("AHN'S AI Assistant")
st.markdown("**Enterprise Document Intelligence Platform**")
st.markdown("---")

def get_fallback_response(prompt: str, context: str = "", error_msg: str = "") -> str:
    """API ì‹¤íŒ¨ì‹œ í´ë°± ì‘ë‹µ"""
    
    # ìì£¼ ë¬»ëŠ” ì§ˆë¬¸ì— ëŒ€í•œ ì§ì ‘ ë‹µë³€
    prompt_lower = prompt.lower()
    
    if any(word in prompt_lower for word in ['ì™€ì´íŒŒì´', 'wifi', 'ë¬´ì„ ']):
        if context and 'pstorm' in context.lower():
            return """ğŸ“¶ **ì™€ì´íŒŒì´ ì •ë³´**

ë„¤íŠ¸ì›Œí¬ëª…: Pstorm_Office
ë¹„ë°€ë²ˆí˜¸: Pstorm#2023
ID: pstorm2019@gmail.com

ë³´ì•ˆ: WPA2-PSK
ëŒ€ì—­í­: 2.4GHz/5GHz ë“€ì–¼ë°´ë“œ

ğŸ’¡ ìœ„ ì •ë³´ë¡œ ë¬´ì„  ë„¤íŠ¸ì›Œí¬ì— ì—°ê²°í•˜ì„¸ìš”!"""
    
    elif any(word in prompt_lower for word in ['adobe', 'ì–´ë„ë¹„']):
        return """ğŸ¨ **Adobe ê´€ë ¨ ì •ë³´**

Adobe ê³„ì •ì´ë‚˜ ë¼ì´ì„ ìŠ¤ ì •ë³´ë¥¼ ì°¾ê³  ê³„ì‹œëŠ”êµ°ìš”.
ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ í™•ì¸í•´ ë³´ê² ìŠµë‹ˆë‹¤.

ğŸ’¡ ë” ì •í™•í•œ ì •ë³´ë¥¼ ìœ„í•´ ë¬¸ì„œë¥¼ ì—…ë°ì´íŠ¸í•˜ê±°ë‚˜ ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”."""
    
    elif any(word in prompt_lower for word in ['gmail', 'êµ¬ê¸€', 'google']):
        return """ğŸ“§ **Gmail ê´€ë ¨ ì •ë³´**

Gmail ê³„ì • ì •ë³´ë¥¼ ì°¾ê³  ê³„ì‹œëŠ”êµ°ìš”.
ë³´ì•ˆì„ ìœ„í•´ ì •í™•í•œ ê³„ì • ì •ë³´ëŠ” ë¬¸ì„œë¥¼ ì§ì ‘ í™•ì¸í•´ì£¼ì„¸ìš”.

ğŸ’¡ ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ê±°ë‚˜ ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”."""
    
    # ì»¨í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ ê°„ë‹¨í•œ ë‹µë³€ ì œê³µ
    elif context.strip():
        return f"""ğŸ“‹ **ë¬¸ì„œ ê¸°ë°˜ ë‹µë³€**

ì§ˆë¬¸: {prompt}

ê´€ë ¨ ë¬¸ì„œ ë‚´ìš©:
{context[:300]}{'...' if len(context) > 300 else ''}

âš ï¸ AI ì„œë¹„ìŠ¤ ì—°ê²°ì— ë¬¸ì œê°€ ìˆì–´ ê°„ë‹¨í•œ ì •ë³´ë§Œ ì œê³µí•©ë‹ˆë‹¤.
ë” ìì„¸í•œ ë¶„ì„ì„ ì›í•˜ì‹œë©´ ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.

ì˜¤ë¥˜ ì •ë³´: {error_msg}"""
    
    # ì¼ë°˜ì ì¸ ì‘ë‹µ
    else:
        return f"""ğŸ¤– **ì‹œìŠ¤í…œ ì•Œë¦¼**

ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ AI ì‘ë‹µ ì„œë¹„ìŠ¤ì— ì¼ì‹œì ì¸ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.

**ë¬¸ì œ**: {error_msg}

**í•´ê²° ë°©ë²•**:
1. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”
2. ë¬¸ì„œì—ì„œ ì§ì ‘ ì •ë³´ë¥¼ ì°¾ì•„ë³´ì„¸ìš”
3. ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì„ í™•ì¸í•´ì£¼ì„¸ìš”

ì§ˆë¬¸í•˜ì‹  ë‚´ìš©: "{prompt}"

ğŸ’¡ ì„œë¹„ìŠ¤ê°€ ë³µêµ¬ë˜ë©´ ë” ìì„¸í•œ ë‹µë³€ì„ ë“œë¦¬ê² ìŠµë‹ˆë‹¤."""

# Luxia API ì„¤ì •
LUXIA_API_KEY = "U2FsdGVkX19ZW0c+KOFb9zDy5eoyiz+I6icUKb2uOjuvUnzY1TaixWa5Ouy0s87vCdtqiQMmScIWcRbEJWcfXt/jS6RMWCW+38TU47bpj82JdafHt3ODi9VHfPmSrZJCMTwP4BJ471NZTqTLakFLpMQ/PTjafRebBJpfLSDeyBj4fX1VM+NnoH8u8aGG5AV4"

# Luxia API ì„¤ì • - ì‹¤ì œ API í‚¤ ì‚¬ìš©
LUXIA_API_KEY = "lZTqTLakFLpMQ/PTjafRebBJpfLSDeyBj4fXiVM+NnoH8u8aGG5AV4"

def get_luxia_response(prompt: str, context: str = "") -> str:
    """Luxia APIë¥¼ í†µí•œ ë‹µë³€ ìƒì„± - ì •í™•í•œ ê³µì‹ ì—”ë“œí¬ì¸íŠ¸ ì‚¬ìš©"""
    try:
        # ê³µì‹ Luxia API ì—”ë“œí¬ì¸íŠ¸
        url = "https://bridge.luxiacloud.com/luxia/v1/chat"
        
        headers = {
            "apikey": LUXIA_API_KEY,  # 'Authorization: Bearer' ëŒ€ì‹  'apikey' ì‚¬ìš©
            "Content-Type": "application/json"
        }
        
        # ì»¨í…ìŠ¤íŠ¸ì™€ ì§ˆë¬¸ì„ ê²°í•©
        if context.strip():
            full_prompt = f"""ë‹¤ìŒ ë¬¸ì„œ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ìì„¸í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”:

[ë¬¸ì„œ ì •ë³´]
{context}

[ì§ˆë¬¸]
{prompt}

ë‹µë³€ì€ ì¹œì ˆí•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”."""
        else:
            full_prompt = prompt

        # Luxia ê³µì‹ API í˜•ì‹
        payload = {
            "model": "luxia2.5-llm-32b-0401",  # ê³µì‹ ëª¨ë¸ëª…
            "messages": [
                {
                    "role": "user",
                    "content": full_prompt
                }
            ]
        }
        
        st.info("ğŸš€ Luxia API ì—°ê²° ì¤‘...")
        
        response = requests.post(
            url, 
            json=payload, 
            headers=headers, 
            timeout=30
        )
        
        if response.status_code == 200:
            result = response.json()
            
            # Luxia API ì‘ë‹µ ì²˜ë¦¬
            if 'choices' in result and len(result['choices']) > 0:
                message_content = result['choices'][0].get('message', {}).get('content', '')
                if message_content:
                    st.success("âœ… Luxia API ì—°ê²° ì„±ê³µ!")
                    return message_content
            
            # ëŒ€ì²´ ì‘ë‹µ í˜•ì‹ ì²˜ë¦¬
            elif 'message' in result:
                st.success("âœ… Luxia API ì—°ê²° ì„±ê³µ!")
                return result['message']
            
            elif 'response' in result:
                st.success("âœ… Luxia API ì—°ê²° ì„±ê³µ!")
                return result['response']
            
            else:
                st.error(f"ì˜ˆìƒì¹˜ ëª»í•œ ì‘ë‹µ í˜•ì‹: {result}")
                return "API ì‘ë‹µ í˜•ì‹ì„ ì¸ì‹í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        elif response.status_code == 401:
            st.error("ğŸ”‘ API í‚¤ ì¸ì¦ ì‹¤íŒ¨ - platform.luxiacloud.comì—ì„œ í‚¤ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”")
            return "API í‚¤ ì¸ì¦ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ëŒ€ì‹œë³´ë“œì—ì„œ API í‚¤ ìƒíƒœë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."
            
        elif response.status_code == 429:
            st.warning("â° API ì‚¬ìš© í•œë„ ì´ˆê³¼ - ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”")
            return "API ì‚¬ìš© í•œë„ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
            
        elif response.status_code == 400:
            st.error(f"âŒ ìš”ì²­ ì˜¤ë¥˜ (400): {response.text}")
            return "ìš”ì²­ í˜•ì‹ì— ì˜¤ë¥˜ê°€ ìˆìŠµë‹ˆë‹¤. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”."
            
        else:
            st.error(f"âŒ API ì˜¤ë¥˜ ({response.status_code}): {response.text}")
            return f"API ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ìƒíƒœ ì½”ë“œ: {response.status_code}"
            
    except requests.exceptions.ConnectionError:
        st.error("ğŸ”Œ ë„¤íŠ¸ì›Œí¬ ì—°ê²° ì˜¤ë¥˜ - ì¸í„°ë„· ì—°ê²°ì„ í™•ì¸í•´ì£¼ì„¸ìš”")
        return "ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ì¸í„°ë„· ì—°ê²°ì„ í™•ì¸í•´ì£¼ì„¸ìš”."
        
    except requests.exceptions.Timeout:
        st.warning("â° ì‘ë‹µ ì‹œê°„ ì´ˆê³¼ - ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”")
        return "ì‘ë‹µ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        
    except Exception as e:
        st.error(f"âŒ ì‹œìŠ¤í…œ ì˜¤ë¥˜: {str(e)}")
        return f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if 'messages' not in st.session_state:
    st.session_state.messages = []

if 'documents' not in st.session_state:
    st.session_state.documents = []

if 'embeddings' not in st.session_state:
    st.session_state.embeddings = None

if 'encoder' not in st.session_state:
    st.session_state.encoder = None

if 'default_loaded' not in st.session_state:
    st.session_state.default_loaded = False

# ë¬¸ì„œ ì²˜ë¦¬ í•¨ìˆ˜ë“¤
def extract_text_from_pdf(file):
    """PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    try:
        pdf_reader = PyPDF2.PdfReader(io.BytesIO(file.read()))
        text = ""
        for page in pdf_reader.pages:
            text += page.extract_text() + "\n"
        return text
    except Exception as e:
        st.error(f"PDF reading error: {e}")
        return ""

def extract_text_from_docx(file):
    """DOCXì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    try:
        doc = Document(io.BytesIO(file.read()))
        text = ""
        for paragraph in doc.paragraphs:
            text += paragraph.text + "\n"
        return text
    except Exception as e:
        st.error(f"DOCX reading error: {e}")
        return ""

def extract_text_from_txt(file):
    """TXTì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    try:
        return file.read().decode('utf-8')
    except Exception as e:
        st.error(f"TXT reading error: {e}")
        return ""

def split_text_into_chunks(text, chunk_size=500):
    """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• """
    if not text.strip():
        return []
    
    sentences = text.replace('\n', ' ').split('. ')
    chunks = []
    current_chunk = []
    current_length = 0
    
    for sentence in sentences:
        sentence = sentence.strip()
        if not sentence:
            continue
            
        sentence_length = len(sentence)
        
        if current_length + sentence_length > chunk_size and current_chunk:
            chunks.append('. '.join(current_chunk) + '.')
            current_chunk = [sentence]
            current_length = sentence_length
        else:
            current_chunk.append(sentence)
            current_length += sentence_length
    
    if current_chunk:
        chunks.append('. '.join(current_chunk) + '.')
    
    return [chunk for chunk in chunks if len(chunk.strip()) > 50]

def process_documents(files):
    """ì—…ë¡œë“œëœ ë¬¸ì„œë“¤ ì²˜ë¦¬"""
    documents = []
    
    for file in files:
        try:
            if file.type == "application/pdf":
                text = extract_text_from_pdf(file)
            elif file.type == "application/vnd.openxmlformats-officedocument.wordprocessingml.document":
                text = extract_text_from_docx(file)
            elif file.type == "text/plain":
                text = extract_text_from_txt(file)
            else:
                st.warning(f"Unsupported file format: {file.name}")
                continue
            
            if not text.strip():
                st.warning(f"No text extracted from: {file.name}")
                continue
            
            chunks = split_text_into_chunks(text, chunk_size=500)
            
            for i, chunk in enumerate(chunks):
                documents.append({
                    'id': f"{file.name}_{i}",
                    'text': chunk,
                    'filename': file.name,
                    'chunk_id': i
                })
        
        except Exception as e:
            st.error(f"Error processing {file.name}: {e}")
    
    return documents

def fallback_keyword_search(query, documents, n_results=3):
    """ì„ë² ë”© ì‹¤íŒ¨ì‹œ í‚¤ì›Œë“œ ê²€ìƒ‰ìœ¼ë¡œ í´ë°±"""
    try:
        if not documents:
            return []
        
        query_lower = query.lower()
        scored_docs = []
        
        for doc in documents:
            text_lower = doc['text'].lower()
            score = 0
            
            # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
            query_words = query_lower.split()
            for word in query_words:
                if len(word) > 1:  # í•œ ê¸€ì ë‹¨ì–´ ì œì™¸
                    score += text_lower.count(word)
            
            if score > 0:
                scored_docs.append((doc['text'], score))
        
        # ì ìˆ˜ë³„ ì •ë ¬
        scored_docs.sort(key=lambda x: x[1], reverse=True)
        
        # ìƒìœ„ ê²°ê³¼ ë°˜í™˜
        results = [doc[0] for doc in scored_docs[:n_results]]
        
        if results:
            st.info("í‚¤ì›Œë“œ ê²€ìƒ‰ìœ¼ë¡œ ëŒ€ì²´í•˜ì—¬ ê²°ê³¼ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
        
        return results
    
    except Exception as e:
        st.error(f"í´ë°± ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return []

@st.cache_resource
def load_sentence_transformer():
    """SentenceTransformer ëª¨ë¸ ë¡œë“œ - ì˜¤ë¥˜ ì²˜ë¦¬ ê°•í™”"""
    try:
        st.info("SentenceTransformer ëª¨ë¸ì„ ë¡œë”©í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
        model = SentenceTransformer('all-MiniLM-L6-v2')
        
        # ëª¨ë¸ì´ ì œëŒ€ë¡œ ë¡œë“œë˜ì—ˆëŠ”ì§€ í…ŒìŠ¤íŠ¸
        test_encoding = model.encode(["test"])
        if test_encoding is not None and len(test_encoding) > 0:
            st.success("âœ… SentenceTransformer ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")
            return model
        else:
            raise Exception("ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì¸ì½”ë”© ì‹¤íŒ¨")
            
    except Exception as e:
        st.error(f"SentenceTransformer ë¡œë”© ì‹¤íŒ¨: {e}")
        st.warning("âš ï¸ ì„ë² ë”© ê²€ìƒ‰ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤. í‚¤ì›Œë“œ ê²€ìƒ‰ìœ¼ë¡œ ëŒ€ì²´ë©ë‹ˆë‹¤.")
        return None

def create_embeddings(documents):
    """ë¬¸ì„œ ì„ë² ë”© ìƒì„± - ì•ˆì „ì„± ê°•í™”"""
    if not documents:
        st.warning("ì²˜ë¦¬í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None, None
    
    try:
        st.info("ì„ë² ë”© ëª¨ë¸ì„ ë¡œë“œí•˜ê³  ìˆìŠµë‹ˆë‹¤...")
        encoder = load_sentence_transformer()
        
        if encoder is None:
            st.warning("ì„ë² ë”© ëª¨ë¸ ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. í‚¤ì›Œë“œ ê²€ìƒ‰ìœ¼ë¡œ ëŒ€ì²´ë©ë‹ˆë‹¤.")
            return None, None
        
        st.info(f"{len(documents)}ê°œ ë¬¸ì„œì˜ ì„ë² ë”©ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
        texts = [doc['text'] for doc in documents]
        
        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬í•˜ì—¬ ë©”ëª¨ë¦¬ ì ˆì•½
        batch_size = 32
        all_embeddings = []
        
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i+batch_size]
            batch_embeddings = encoder.encode(batch_texts, show_progress_bar=False)
            all_embeddings.extend(batch_embeddings)
        
        embeddings = np.array(all_embeddings)
        st.success(f"âœ… {len(embeddings)}ê°œ ë¬¸ì„œ ì„ë² ë”© ìƒì„± ì™„ë£Œ!")
        
        return embeddings, encoder
    
    except Exception as e:
        st.error(f"ì„ë² ë”© ìƒì„± ì˜¤ë¥˜: {e}")
        st.warning("í‚¤ì›Œë“œ ê²€ìƒ‰ìœ¼ë¡œ ëŒ€ì²´ë©ë‹ˆë‹¤.")
        return None, None

def search_documents(query, documents, embeddings, encoder, n_results=3):
    """ë¬¸ì„œì—ì„œ ê´€ë ¨ ë‚´ìš© ê²€ìƒ‰"""
    try:
        # None ì²´í¬ ê°•í™”
        if not documents or embeddings is None or encoder is None:
            st.warning("ê²€ìƒ‰ ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¬¸ì„œë¥¼ ë‹¤ì‹œ ì²˜ë¦¬í•´ì£¼ì„¸ìš”.")
            return []
        
        # encoderê°€ ì œëŒ€ë¡œ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸
        if not hasattr(encoder, 'encode'):
            st.error("SentenceTransformer ëª¨ë¸ì´ ì œëŒ€ë¡œ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return []
        
        query_embedding = encoder.encode([query])
        similarities = cosine_similarity(query_embedding, embeddings)[0]
        top_indices = np.argsort(similarities)[::-1][:n_results]
        
        results = []
        for idx in top_indices:
            if similarities[idx] > 0.1:
                results.append(documents[idx]['text'])
        
        return results
    
    except Exception as e:
        st.error(f"Document search error: {e}")
        # í´ë°±: í‚¤ì›Œë“œ ê²€ìƒ‰ìœ¼ë¡œ ëŒ€ì²´
        return fallback_keyword_search(query, documents, n_results)

def load_default_document():
    """GitHubì—ì„œ ê¸°ë³¸ ë¬¸ì„œ ë¡œë“œ (password-rag-agent2 ê²½ë¡œ ì‚¬ìš©)"""
    try:
        # GitHub raw íŒŒì¼ URL - password-rag-agent2ë¡œ ë³€ê²½
        github_url = "https://raw.githubusercontent.com/ppacksae/password-rag-agent2/main/pstorm_pw.docx"
        
        # íŒŒì¼ ë‹¤ìš´ë¡œë“œ
        response = requests.get(github_url, timeout=10)
        
        if response.status_code == 200:
            # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
            with tempfile.NamedTemporaryFile(delete=False, suffix='.docx') as tmp_file:
                tmp_file.write(response.content)
                tmp_file_path = tmp_file.name
            
            try:
                # DOCX íŒŒì¼ ì½ê¸°
                doc = Document(tmp_file_path)
                content = ""
                for paragraph in doc.paragraphs:
                    content += paragraph.text + "\n"
                
                # ì„ì‹œ íŒŒì¼ ì‚­ì œ
                os.unlink(tmp_file_path)
                
                if not content.strip():
                    raise Exception("ë¬¸ì„œê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
                
            except Exception as e:
                # ì„ì‹œ íŒŒì¼ ì‚­ì œ (ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë„)
                if os.path.exists(tmp_file_path):
                    os.unlink(tmp_file_path)
                raise e
        else:
            raise Exception(f"GitHubì—ì„œ íŒŒì¼ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Status: {response.status_code}")
        
        # í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• 
        chunks = split_text_into_chunks(content, chunk_size=500)
        
        documents = []
        for i, chunk in enumerate(chunks):
            documents.append({
                'id': f"pstorm_pw.docx_{i}",
                'text': chunk,
                'filename': "pstorm_pw.docx",
                'chunk_id': i
            })
        
        return documents
    
    except Exception as e:
        st.error(f"GitHubì—ì„œ ê¸°ë³¸ ë¬¸ì„œ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
        st.info("ê¸°ë³¸ ë‚´ìš©ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
        
        # í´ë°± - ê¸°ë³¸ ë‚´ìš© ì‚¬ìš©
        default_content = """
6. ì™€ì´íŒŒì´(WIFI)
1) ë¹„ë²ˆ(password) : Pstorm#2023
2) ID: pstorm2019@gmail.com

ë„¤íŠ¸ì›Œí¬ ì„¤ì •:
- ë„¤íŠ¸ì›Œí¬ëª…: Pstorm_Office
- ë³´ì•ˆ: WPA2-PSK
- ëŒ€ì—­í­: 2.4GHz/5GHz ë“€ì–¼ë°´ë“œ
- ìµœëŒ€ ì—°ê²° ê¸°ê¸°: 50ëŒ€

ê´€ë¦¬ì ì •ë³´:
- ê´€ë¦¬ì ID: admin
- ê´€ë¦¬ì ë¹„ë°€ë²ˆí˜¸: admin123!
- ì›¹ ê´€ë¦¬ ì£¼ì†Œ: 192.168.1.1

ì¶”ê°€ ì •ë³´:
- ê²ŒìŠ¤íŠ¸ ë„¤íŠ¸ì›Œí¬: Pstorm_Guest
- ê²ŒìŠ¤íŠ¸ ë¹„ë°€ë²ˆí˜¸: guest2023
- í¬íŠ¸ í¬ì›Œë”©: í™œì„±í™”
- ë°©í™”ë²½: ê¸°ë³¸ ì„¤ì •
"""
        
        chunks = split_text_into_chunks(default_content, chunk_size=500)
        documents = []
        for i, chunk in enumerate(chunks):
            documents.append({
                'id': f"pstorm_pw.docx_{i}",
                'text': chunk,
                'filename': "pstorm_pw.docx",
                'chunk_id': i
            })
        
        return documents

def generate_response(query, context_docs):
    """Luxiaë¥¼ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µ ìƒì„±"""
    try:
        if context_docs:
            context = "\n\n".join(context_docs)
            response = get_luxia_response(query, context)
        else:
            no_docs_prompt = f"""
ì—…ë¡œë“œëœ ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

ì§ˆë¬¸: {query}

ì¼ë°˜ì ì¸ ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ ì œê³µí•˜ê² ìŠµë‹ˆë‹¤ë§Œ, ë¨¼ì € ì—…ë¡œë“œëœ ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆë‹¤ëŠ” ì ì„ ë§ì”€ë“œë¦½ë‹ˆë‹¤.
"""
            response = get_luxia_response(no_docs_prompt)
        
        return response
    
    except Exception as e:
        return f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

# ì‚¬ì´ë“œë°” ì„¤ì •
with st.sidebar:
    st.header("Configuration")
    
    # API í‚¤ ì •ë³´ í‘œì‹œ (ì½ê¸° ì „ìš©)
    st.markdown("### ğŸ”‘ AI Model")
    st.success("ğŸš€ **Luxia Platform** ì—°ê²° ì¤€ë¹„ë¨")
    
    st.markdown("---")
    
    # ë¬¸ì„œ ê´€ë¦¬ ì„¹ì…˜
    st.header("Document Management")
    
    # ë¬¸ì„œ ì—…ë¡œë“œ
    st.subheader("Upload Documents")
    uploaded_files = st.file_uploader(
        "Supported formats: PDF, DOCX, TXT",
        type=['pdf', 'docx', 'txt'],
        accept_multiple_files=True,
        help="Upload company documents for AI analysis"
    )
    
    # ê¸°ë³¸ ë¬¸ì„œ ìë™ ë¡œë“œ
    if not st.session_state.default_loaded and not st.session_state.documents:
        with st.spinner("ê¸°ë³¸ ë¬¸ì„œë¥¼ ë¡œë“œí•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
            default_docs = load_default_document()
            if default_docs:
                st.session_state.documents = default_docs
                
                # ì„ë² ë”© ìƒì„±
                embeddings, encoder = create_embeddings(default_docs)
                if embeddings is not None:
                    st.session_state.embeddings = embeddings
                    st.session_state.encoder = encoder
                    st.session_state.default_loaded = True
                    st.success("âœ… ê¸°ë³¸ ë¬¸ì„œ (pstorm_pw.docx) ë¡œë“œ ì™„ë£Œ!")
                    st.rerun()
    
    # ë¬¸ì„œ ì²˜ë¦¬ ë²„íŠ¼
    if uploaded_files:
        if st.button("Process Documents", type="primary", use_container_width=True):
            with st.spinner("Processing documents..."):
                # ë¬¸ì„œ ì²˜ë¦¬ ë¡œì§
                documents = process_documents(uploaded_files)
                
                if documents:
                    st.session_state.documents = documents
                    
                    with st.spinner("Generating embeddings..."):
                        embeddings, encoder = create_embeddings(documents)
                        if embeddings is not None:
                            st.session_state.embeddings = embeddings
                            st.session_state.encoder = encoder
                            st.success(f"Processed {len(documents)} document chunks")
                        else:
                            st.error("Embedding generation failed")
                else:
                    st.warning("No processable documents found")
                
                st.rerun()
    
    st.markdown("---")
    
    # ë¬¸ì„œ í˜„í™©
    st.subheader("Document Status")
    if st.session_state.get('documents'):
        st.metric("Total Chunks", len(st.session_state.documents))
        
        # íŒŒì¼ë³„ ì²­í¬ ìˆ˜ í‘œì‹œ
        file_counts = {}
        for doc in st.session_state.documents:
            filename = doc['filename']
            file_counts[filename] = file_counts.get(filename, 0) + 1
        
        for filename, count in file_counts.items():
            st.text(f"{filename}: {count} chunks")
        
        # ê²€ìƒ‰ ê¸°ëŠ¥ ìƒíƒœ
        if st.session_state.get('embeddings') is not None:
            st.success("ğŸ” Vector Search: Active")
        elif st.session_state.get('documents'):
            st.info("ğŸ”¤ Keyword Search: Active")
        else:
            st.warning("âŒ Search: Inactive")
    else:
        st.info("No documents loaded")
    
    st.markdown("---")
    
    # ê´€ë¦¬ ê¸°ëŠ¥
    st.subheader("System Management")
    
    col1, col2 = st.columns(2)
    
    with col1:
        if st.button("Clear Chat", use_container_width=True):
            st.session_state.messages = []
            st.rerun()
    
    with col2:
        if st.button("Clear Docs", use_container_width=True):
            st.session_state.documents = []
            st.session_state.embeddings = None
            st.session_state.encoder = None
            st.rerun()

# ì‚¬ìš©ë²• ì•ˆë‚´ë¥¼ ì‚¬ì´ë“œë°”ë¡œ ì´ë™
with st.sidebar:
    st.markdown("---")
    st.subheader("System Information")
    
    with st.expander("Getting Started"):
        st.markdown("""
        1. AI is powered by Luxia (already connected)
        2. Upload documents using the file uploader
        3. Click "Process Documents" to enable AI search
        4. Ask questions about your documents in the chat
        """)
    
    with st.expander("Features"):
        st.markdown("""
        - PDF, DOCX, TXT file support
        - Multiple file upload capability
        - Vector-based document search
        - Professional AI responses
        - Source document references
        """)
    
    with st.expander("Tips"):
        st.markdown("""
        - Use specific questions for better results
        - Multiple files can be processed together
        - First document processing may take time
        - Referenced sources shown below responses
        """)

# ë©”ì¸ ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
st.header("AI Chat Interface")

# ì±„íŒ… ì»¨í…Œì´ë„ˆ (ì‚¬ì´ë“œë°” ë„ˆë¹„ë§Œí¼ ì—¬ë°± ì¶”ê°€)
chat_container = st.container()

with chat_container:
    # ì´ˆê¸° í™˜ì˜ ë©”ì‹œì§€ (ì±„íŒ…ì´ ë¹„ì–´ìˆì„ ë•Œë§Œ í‘œì‹œ)
    if not st.session_state.messages:
        st.markdown("""
        <div style="
            display: flex;
            justify-content: center;
            align-items: center;
            height: 300px;
            flex-direction: column;
            margin-left: 0;
        ">
            <h1 style="
                font-size: 3rem;
                font-weight: 300;
                color: #2c3e50;
                margin-bottom: 2rem;
                text-align: center;
            ">ì•ˆë…•í•˜ì„¸ìš”</h1>
            <p style="
                font-size: 1.2rem;
                color: #7f8c8d;
                text-align: center;
                margin-bottom: 3rem;
            ">AHN'S AI Assistantê°€ ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤</p>
        </div>
        """, unsafe_allow_html=True)

    # ì±„íŒ… ë©”ì‹œì§€ í‘œì‹œ (ì»¤ìŠ¤í…€ ìŠ¤íƒ€ì¼)
    if st.session_state.messages:
        for i, message in enumerate(st.session_state.messages):
            if message["role"] == "user":
                # ì‚¬ìš©ì ë©”ì‹œì§€ - ìš°ì¸¡ ì •ë ¬
                st.markdown(f"""
                <div style="
                    display: flex;
                    justify-content: flex-end;
                    margin: 1rem 0;
                    padding-right: 1rem;
                ">
                    <div style="
                        background-color: #e3f2fd;
                        color: #1565c0;
                        padding: 0.8rem 1.2rem;
                        border-radius: 18px 18px 4px 18px;
                        max-width: 70%;
                        font-size: 0.95rem;
                        line-height: 1.4;
                        box-shadow: 0 1px 2px rgba(0,0,0,0.1);
                        word-wrap: break-word;
                    ">
                        {message["content"]}
                    </div>
                </div>
                """, unsafe_allow_html=True)
            else:
                # AI ì‘ë‹µ - ì¢Œì¸¡ ì •ë ¬
                # HTML íƒœê·¸ ì œê±°í•˜ê³  ê¹”ë”í•˜ê²Œ í‘œì‹œ
                clean_content = message["content"].replace('<div>', '').replace('</div>', '').strip()
                
                st.markdown(f"""
                <div style="
                    display: flex;
                    justify-content: flex-start;
                    margin: 1rem 0;
                    align-items: flex-start;
                    padding-left: 1rem;
                ">
                    <div style="
                        background-color: #f5f5f5;
                        color: #2c3e50;
                        padding: 0.8rem 1.2rem;
                        border-radius: 18px 18px 18px 4px;
                        max-width: 75%;
                        font-size: 0.95rem;
                        line-height: 1.5;
                        box-shadow: 0 1px 2px rgba(0,0,0,0.1);
                        border: 1px solid #e9ecef;
                        word-wrap: break-word;
                        white-space: pre-wrap;
                    ">
                        ğŸ¤– {clean_content}
                    </div>
                </div>
                """, unsafe_allow_html=True)
                
                # ì°¸ê³  ë¬¸ì„œê°€ ìˆì„ ê²½ìš° í‘œì‹œ
                if "references" in message:
                    with st.expander(f"ğŸ“š ì°¸ê³ í•œ ë¬¸ì„œ ({len(message['references'])}ê°œ)"):
                        for j, doc in enumerate(message["references"]):
                            st.write(f"**ë¬¸ì„œ {j+1}:**")
                            st.write(doc[:200] + "..." if len(doc) > 200 else doc)
                            if j < len(message["references"]) - 1:
                                st.markdown("---")

# ì»¤ìŠ¤í…€ ì±„íŒ… ì…ë ¥ì°½
st.markdown("""
<style>
    /* ë©”ì¸ ì»¨í…ì¸  ì˜ì—­ ì‚¬ì´ë“œë°” ê²¹ì¹¨ ë°©ì§€ */
    .main .block-container {
        padding-bottom: 120px !important;
        padding-left: 1rem !important;
        padding-right: 1rem !important;
    }
    
    /* ì‚¬ì´ë“œë°”ê°€ ìˆì„ ë•Œ ë©”ì¸ ì½˜í…ì¸  ì—¬ë°± ì¡°ì • */
    .main {
        margin-left: 0 !important;
    }
    
    /* ì±„íŒ… ì»¨í…Œì´ë„ˆ ìŠ¤íƒ€ì¼ */
    .chat-container {
        margin-left: 0;
        width: 100%;
        max-width: none;
    }
    
    /* ì±„íŒ… ì…ë ¥ì°½ ì»¤ìŠ¤í„°ë§ˆì´ì§• */
    .stChatInput {
        position: fixed;
        bottom: 0;
        left: 320px; /* ì‚¬ì´ë“œë°” ë„ˆë¹„ë§Œí¼ ì—¬ë°± */
        right: 0;
        background: white;
        border-top: 1px solid #e9ecef;
        padding: 1rem;
        z-index: 999;
        box-shadow: 0 -2px 10px rgba(0,0,0,0.1);
    }
    
    /* ì‚¬ì´ë“œë°”ê°€ ì¶•ì†Œëœ ê²½ìš° */
    .css-1lcbmhc.e1fqkh3o0 + .main .stChatInput {
        left: 60px;
    }
    
    [data-testid="stChatInput"] {
        margin-bottom: 0;
        max-width: calc(100vw - 360px); /* ì‚¬ì´ë“œë°” ê³ ë ¤í•œ ìµœëŒ€ ë„ˆë¹„ */
    }
    
    [data-testid="stChatInput"] textarea {
        border: 2px solid #e9ecef !important;
        border-radius: 25px !important;
        padding: 12px 20px !important;
        font-size: 1rem !important;
        resize: none !important;
        box-shadow: 0 2px 10px rgba(0,0,0,0.1) !important;
        transition: all 0.2s ease !important;
        width: 100% !important;
    }
    
    [data-testid="stChatInput"] textarea:focus {
        border-color: #3498db !important;
        box-shadow: 0 0 0 3px rgba(52, 152, 219, 0.1), 0 2px 10px rgba(0,0,0,0.15) !important;
        outline: none !important;
    }
    
    [data-testid="stChatInput"] textarea::placeholder {
        color: #7f8c8d !important;
        font-size: 1rem !important;
    }
    
    /* ì±„íŒ… ë©”ì‹œì§€ ì˜ì—­ ì—¬ë°± */
    .element-container:has([data-testid="stChatInput"]) {
        margin-bottom: 80px;
    }
    
    /* ëª¨ë°”ì¼ ëŒ€ì‘ */
    @media (max-width: 768px) {
        .stChatInput {
            left: 0;
        }
        [data-testid="stChatInput"] {
            max-width: 100vw;
        }
    }
</style>
""", unsafe_allow_html=True)

# ì‚¬ìš©ì ì…ë ¥ (ì»¤ìŠ¤í…€ placeholder)
if prompt := st.chat_input("AHN'S AI ì—ê²Œ ë¬¼ì–´ë³´ê¸°"):
    # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
    st.session_state.messages.append({"role": "user", "content": prompt})
    
    # AI ì‘ë‹µ ìƒì„±
    with st.spinner("Luxia AIê°€ ë‹µë³€ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
        # ë¬¸ì„œ ê²€ìƒ‰
        relevant_docs = search_documents(
            prompt, 
            st.session_state.documents, 
            st.session_state.embeddings, 
            st.session_state.encoder
        )
        
        # ì‘ë‹µ ìƒì„±
        response = generate_response(prompt, relevant_docs)
        
        # ì‘ë‹µì„ ì„¸ì…˜ì— ì €ì¥ (ì°¸ê³  ë¬¸ì„œ í¬í•¨)
        message_data = {"role": "assistant", "content": response}
        if relevant_docs:
            message_data["references"] = relevant_docs
        
        st.session_state.messages.append(message_data)
        
    # í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨ìœ¼ë¡œ UI ì—…ë°ì´íŠ¸
    st.rerun()

# í‘¸í„°
st.markdown("---")
st.markdown("**AHN'S AI Assistant** | Enterprise Document Intelligence Platform | Powered by Luxia AI")