import streamlit as st
import google.generativeai as genai
import PyPDF2
from docx import Document
import io
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer
import pandas as pd
import requests
import tempfile
import os
import re
from typing import List, Tuple, Dict
import json

# Streamlit í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="Advanced RAG Assistant v2",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ë‹¤í¬ëª¨ë“œ CSS ìŠ¤íƒ€ì¼ë§ (Luxia-ON ìŠ¤íƒ€ì¼)
def apply_dark_theme():
    st.markdown("""
    <style>
    /* ì „ì²´ ë°°ê²½ */
    .stApp {
        background: linear-gradient(135deg, #1e3c72 0%, #2a5298 100%);
        color: #ffffff;
    }
    
    /* ë©”ì¸ ì»¨í…Œì´ë„ˆ */
    .main-container {
        background: rgba(0, 0, 0, 0.7);
        border-radius: 20px;
        padding: 30px;
        margin: 20px;
        backdrop-filter: blur(10px);
        border: 1px solid rgba(255, 255, 255, 0.1);
    }
    
    /* í—¤ë” ìŠ¤íƒ€ì¼ */
    .header-container {
        text-align: center;
        margin-bottom: 30px;
    }
    
    .app-title {
        font-size: 2.5rem;
        font-weight: bold;
        background: linear-gradient(45deg, #00d4ff, #ff6b6b);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        margin-bottom: 10px;
    }
    
    .app-subtitle {
        color: #a0a0a0;
        font-size: 1.1rem;
        margin-bottom: 20px;
    }
    
    /* ì±„íŒ… ì»¨í…Œì´ë„ˆ */
    .chat-container {
        background: rgba(30, 30, 30, 0.9);
        border-radius: 15px;
        padding: 20px;
        margin: 10px 0;
        border: 1px solid rgba(255, 255, 255, 0.1);
        min-height: 400px;
        max-height: 600px;
        overflow-y: auto;
    }
    
    /* ë©”ì‹œì§€ ìŠ¤íƒ€ì¼ */
    .user-message {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        padding: 15px 20px;
        border-radius: 20px 20px 5px 20px;
        margin: 10px 0;
        margin-left: 20%;
        box-shadow: 0 4px 15px rgba(102, 126, 234, 0.3);
    }
    
    .bot-message {
        background: rgba(50, 50, 50, 0.9);
        color: #ffffff;
        padding: 15px 20px;
        border-radius: 20px 20px 20px 5px;
        margin: 10px 0;
        margin-right: 20%;
        border-left: 4px solid #00d4ff;
        box-shadow: 0 4px 15px rgba(0, 0, 0, 0.3);
    }
    
    /* ì…ë ¥ ì˜ì—­ */
    .input-container {
        background: rgba(30, 30, 30, 0.8);
        border-radius: 25px;
        padding: 10px;
        margin: 20px 0;
        border: 2px solid rgba(0, 212, 255, 0.3);
    }
    
    /* ì‚¬ì´ë“œë°” */
    .css-1d391kg {
        background: rgba(20, 20, 20, 0.9);
    }
    
    /* ë²„íŠ¼ ìŠ¤íƒ€ì¼ */
    .stButton > button {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        border: none;
        border-radius: 25px;
        padding: 12px 30px;
        font-weight: bold;
        transition: all 0.3s ease;
    }
    
    .stButton > button:hover {
        transform: translateY(-2px);
        box-shadow: 0 8px 25px rgba(102, 126, 234, 0.4);
    }
    
    /* ì„ íƒë°•ìŠ¤ */
    .stSelectbox > div > div {
        background: rgba(50, 50, 50, 0.8);
        color: white;
        border-radius: 10px;
    }
    
    /* íŒŒì¼ ì—…ë¡œë” */
    .stFileUploader > div {
        background: rgba(50, 50, 50, 0.8);
        border-radius: 15px;
        border: 2px dashed rgba(0, 212, 255, 0.5);
    }
    
    /* ë©”íŠ¸ë¦­ ì¹´ë“œ */
    .metric-card {
        background: rgba(50, 50, 50, 0.8);
        padding: 15px;
        border-radius: 10px;
        margin: 10px 0;
        border-left: 4px solid #00d4ff;
    }
    
    /* ìŠ¤í¬ë¡¤ë°” */
    ::-webkit-scrollbar {
        width: 8px;
    }
    
    ::-webkit-scrollbar-track {
        background: rgba(255, 255, 255, 0.1);
        border-radius: 10px;
    }
    
    ::-webkit-scrollbar-thumb {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        border-radius: 10px;
    }
    
    /* ìˆ¨ê²¨ì•¼ í•  ìš”ì†Œë“¤ */
    #MainMenu {visibility: hidden;}
    footer {visibility: hidden;}
    header {visibility: hidden;}
    </style>
    """, unsafe_allow_html=True)

# ì „ì—­ ë³€ìˆ˜
if 'conversation' not in st.session_state:
    st.session_state.conversation = []
if 'documents' not in st.session_state:
    st.session_state.documents = []
if 'embeddings' not in st.session_state:
    st.session_state.embeddings = []
if 'model' not in st.session_state:
    st.session_state.model = None
if 'vectorizer' not in st.session_state:
    st.session_state.vectorizer = None
if 'tfidf_matrix' not in st.session_state:
    st.session_state.tfidf_matrix = None

class AdvancedRAGSystem:
    def __init__(self):
        self.embedding_model = None
        self.vectorizer = TfidfVectorizer(
            max_features=10000,
            ngram_range=(1, 2),
            stop_words=None
        )
        
    def load_embedding_model(self):
        """ì„ë² ë”© ëª¨ë¸ ë¡œë“œ"""
        if self.embedding_model is None:
            with st.spinner("ğŸ”„ ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘..."):
                self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
        return self.embedding_model
    
    def chunk_text_with_overlap(self, text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:
        """ì˜¤ë²„ë©ì´ ìˆëŠ” í…ìŠ¤íŠ¸ ì²­í‚¹"""
        if len(text) <= chunk_size:
            return [text]
        
        chunks = []
        start = 0
        
        while start < len(text):
            end = start + chunk_size
            
            # ë¬¸ì¥ ê²½ê³„ì—ì„œ ìë¥´ê¸°
            if end < len(text):
                # ë§ˆì§€ë§‰ ë¬¸ì¥ ëì„ ì°¾ê¸°
                last_period = text.rfind('.', start, end)
                last_exclamation = text.rfind('!', start, end)
                last_question = text.rfind('?', start, end)
                
                sentence_end = max(last_period, last_exclamation, last_question)
                
                if sentence_end > start:
                    end = sentence_end + 1
            
            chunk = text[start:end].strip()
            if chunk:
                chunks.append(chunk)
            
            # ì˜¤ë²„ë© ê³ ë ¤í•œ ë‹¤ìŒ ì‹œì‘ì 
            start = end - overlap if end < len(text) else len(text)
            
            if start >= len(text):
                break
                
        return chunks
    
    def hybrid_search(self, query: str, documents: List[str], embeddings: np.ndarray, 
                     top_k: int = 5, alpha: float = 0.7) -> List[Tuple[str, float]]:
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ë²¡í„° + í‚¤ì›Œë“œ)"""
        if not documents:
            return []
        
        # ë²¡í„° ê²€ìƒ‰
        query_embedding = self.embedding_model.encode([query])
        vector_similarities = cosine_similarity(query_embedding, embeddings)[0]
        
        # í‚¤ì›Œë“œ ê²€ìƒ‰ (TF-IDF)
        try:
            query_tfidf = self.vectorizer.transform([query])
            keyword_similarities = cosine_similarity(query_tfidf, st.session_state.tfidf_matrix)[0]
        except:
            keyword_similarities = np.zeros(len(documents))
        
        # í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê³„ì‚°
        hybrid_scores = alpha * vector_similarities + (1 - alpha) * keyword_similarities
        
        # ìƒìœ„ ë¬¸ì„œ ì„ íƒ
        top_indices = np.argsort(hybrid_scores)[::-1][:top_k]
        
        results = []
        for idx in top_indices:
            if hybrid_scores[idx] > 0:
                results.append((documents[idx], hybrid_scores[idx]))
        
        return results
    
    def rerank_documents(self, query: str, documents: List[Tuple[str, float]], 
                        max_docs: int = 3) -> List[str]:
        """Re-ranking with query relevance"""
        if not documents:
            return []
        
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ re-ranking
        query_terms = set(query.lower().split())
        
        reranked = []
        for doc, score in documents:
            doc_terms = set(doc.lower().split())
            overlap = len(query_terms.intersection(doc_terms))
            
            # í‚¤ì›Œë“œ ì˜¤ë²„ë©ê³¼ ê¸°ì¡´ ì ìˆ˜ ê²°í•©
            final_score = score + (overlap * 0.1)
            reranked.append((doc, final_score))
        
        # ìµœì¢… ì ìˆ˜ë¡œ ì •ë ¬
        reranked.sort(key=lambda x: x[1], reverse=True)
        
        return [doc for doc, _ in reranked[:max_docs]]

# RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
rag_system = AdvancedRAGSystem()

def setup_luxia_api(api_key: str):
    """Luxia API ì„¤ì •"""
    return api_key

def setup_gemini_api(api_key: str):
    """Gemini API ì„¤ì •"""
    genai.configure(api_key=api_key)
    return genai.GenerativeModel('gemini-pro')

def generate_luxia_response(prompt: str, context: str, api_key: str) -> str:
    """Luxia API í˜¸ì¶œ"""
    try:
        headers = {
            'Authorization': f'Bearer {api_key}',
            'Content-Type': 'application/json'
        }
        
        if context:
            full_prompt = f"""ë¬¸ì„œ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

ë¬¸ì„œ ë‚´ìš©:
{context}

ì§ˆë¬¸: {prompt}

ë‹µë³€:"""
        else:
            full_prompt = prompt
        
        data = {
            'model': 'luxia-2.5',
            'messages': [
                {'role': 'user', 'content': full_prompt}
            ],
            'max_tokens': 1000,
            'temperature': 0.7
        }
        
        # Luxia API ì—”ë“œí¬ì¸íŠ¸ (ì‹¤ì œ API ëª…ì„¸ì— ë”°ë¼ ìˆ˜ì • í•„ìš”)
        response = requests.post(
            'https://api.saltlux.com/v1/chat/completions',
            headers=headers,
            json=data,
            timeout=30
        )
        
        if response.status_code == 200:
            result = response.json()
            return result['choices'][0]['message']['content']
        else:
            return f"âŒ Luxia API ì˜¤ë¥˜: {response.status_code} - {response.text}"
            
    except Exception as e:
        return f"âŒ Luxia API ì—°ê²° ì˜¤ë¥˜: {str(e)}"

def generate_gemini_response(prompt: str, context: str, model) -> str:
    """Gemini API í˜¸ì¶œ"""
    try:
        if context:
            full_prompt = f"""ë¬¸ì„œ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

ë¬¸ì„œ ë‚´ìš©:
{context}

ì§ˆë¬¸: {prompt}

ë‹µë³€:"""
        else:
            full_prompt = prompt
        
        response = model.generate_content(full_prompt)
        return response.text
        
    except Exception as e:
        return f"âŒ Gemini API ì˜¤ë¥˜: {str(e)}"

def extract_text_from_pdf(pdf_file):
    """PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    try:
        pdf_reader = PyPDF2.PdfReader(pdf_file)
        text = ""
        for page in pdf_reader.pages:
            text += page.extract_text()
        return text
    except Exception as e:
        st.error(f"PDF ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        return ""

def extract_text_from_docx(docx_file):
    """DOCXì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    try:
        doc = Document(docx_file)
        text = ""
        for paragraph in doc.paragraphs:
            text += paragraph.text + "\n"
        return text
    except Exception as e:
        st.error(f"DOCX ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        return ""

def process_documents(uploaded_files):
    """ì—…ë¡œë“œëœ ë¬¸ì„œë“¤ ì²˜ë¦¬"""
    all_chunks = []
    
    for uploaded_file in uploaded_files:
        if uploaded_file.type == "application/pdf":
            text = extract_text_from_pdf(uploaded_file)
        elif uploaded_file.type == "application/vnd.openxmlformats-officedocument.wordprocessingml.document":
            text = extract_text_from_docx(uploaded_file)
        else:
            text = str(uploaded_file.read(), "utf-8")
        
        if text.strip():
            # ì˜¤ë²„ë©ì´ ìˆëŠ” ì²­í‚¹
            chunks = rag_system.chunk_text_with_overlap(
                text, 
                chunk_size=500, 
                overlap=50
            )
            all_chunks.extend(chunks)
    
    return all_chunks

def main():
    # ë‹¤í¬ í…Œë§ˆ ì ìš©
    apply_dark_theme()
    
    # í—¤ë”
    st.markdown("""
    <div class="header-container">
        <div class="app-title">ğŸ¤– Advanced RAG Assistant v2</div>
        <div class="app-subtitle">Luxia & Gemini ê¸°ë°˜ ì§€ëŠ¥í˜• ë¬¸ì„œ ë¶„ì„ ì‹œìŠ¤í…œ</div>
    </div>
    """, unsafe_allow_html=True)
    
    # ì‚¬ì´ë“œë°” ì„¤ì •
    with st.sidebar:
        st.markdown("### âš™ï¸ ì„¤ì •")
        
        # AI ëª¨ë¸ ì„ íƒ
        ai_model = st.selectbox(
            "ğŸ¤– AI ëª¨ë¸ ì„ íƒ",
            ["Luxia", "Gemini"],
            index=0,  # ê¸°ë³¸ê°’: Luxia
            help="ì‚¬ìš©í•  AI ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”"
        )
        
        # API í‚¤ ì…ë ¥
        if ai_model == "Luxia":
            api_key = st.text_input(
                "ğŸ”‘ Luxia API í‚¤",
                value="U2FsdGVkX19ZW0c+KOFb9zDy5eoyiz+I6icUKb2uOjuvUnzY1TaixWa5Ouy0s87vCdtqiQMmScIWcRbEJWcfXt/jS6RMWCW+38TU47bpj82JdafHt3ODi9VHfPmSrZJCMTwP4BJ471NZTqTLakFLpMQ/PTjafRebBJpfLSDeyBj4fX1VM+NnoH8u8aGG5AV4",
                type="password"
            )
        else:
            api_key = st.text_input(
                "ğŸ”‘ Gemini API í‚¤",
                type="password"
            )
        
        st.markdown("---")
        
        # ê²€ìƒ‰ ì„¤ì •
        st.markdown("### ğŸ” ê²€ìƒ‰ ì„¤ì •")
        
        search_k = st.slider(
            "ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜",
            min_value=1,
            max_value=10,
            value=5,
            help="ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ëª‡ ê°œê¹Œì§€ ê²€ìƒ‰í• ì§€ ì„¤ì •"
        )
        
        alpha = st.slider(
            "í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê°€ì¤‘ì¹˜",
            min_value=0.0,
            max_value=1.0,
            value=0.7,
            step=0.1,
            help="ë²¡í„° ê²€ìƒ‰(1.0) vs í‚¤ì›Œë“œ ê²€ìƒ‰(0.0)"
        )
        
        st.markdown("---")
        
        # ë¬¸ì„œ ì—…ë¡œë“œ
        st.markdown("### ğŸ“„ ë¬¸ì„œ ì—…ë¡œë“œ")
        uploaded_files = st.file_uploader(
            "ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”",
            type=['pdf', 'docx', 'txt'],
            accept_multiple_files=True,
            help="PDF, DOCX, TXT íŒŒì¼ì„ ì§€ì›í•©ë‹ˆë‹¤"
        )
        
        if uploaded_files:
            if st.button("ğŸ“Š ë¬¸ì„œ ì²˜ë¦¬", type="primary"):
                with st.spinner("ğŸ”„ ë¬¸ì„œ ì²˜ë¦¬ ì¤‘..."):
                    # ë¬¸ì„œ ì²˜ë¦¬
                    chunks = process_documents(uploaded_files)
                    st.session_state.documents = chunks
                    
                    if chunks:
                        # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
                        embedding_model = rag_system.load_embedding_model()
                        
                        # ë²¡í„° ì„ë² ë”© ìƒì„±
                        embeddings = embedding_model.encode(chunks)
                        st.session_state.embeddings = embeddings
                        
                        # TF-IDF í–‰ë ¬ ìƒì„±
                        st.session_state.tfidf_matrix = rag_system.vectorizer.fit_transform(chunks)
                        
                        st.success(f"âœ… {len(chunks)}ê°œ ì²­í¬ë¡œ ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ!")
                        
                        # ë¬¸ì„œ í†µê³„
                        st.markdown("### ğŸ“Š ë¬¸ì„œ í†µê³„")
                        col1, col2 = st.columns(2)
                        with col1:
                            st.metric("ì´ ì²­í¬ ìˆ˜", len(chunks))
                        with col2:
                            avg_length = sum(len(chunk) for chunk in chunks) // len(chunks)
                            st.metric("í‰ê·  ì²­í¬ ê¸¸ì´", f"{avg_length}ì")
    
    # ë©”ì¸ ì»¨í…ì¸ 
    if not api_key:
        st.warning("âš ï¸ API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        return
    
    # API ì„¤ì •
    if ai_model == "Luxia":
        model = setup_luxia_api(api_key)
    else:
        model = setup_gemini_api(api_key)
        st.session_state.model = model
    
    # ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
    st.markdown('<div class="chat-container">', unsafe_allow_html=True)
    
    # ëŒ€í™” ê¸°ë¡ í‘œì‹œ
    for i, (role, message) in enumerate(st.session_state.conversation):
        if role == "user":
            st.markdown(f'<div class="user-message">ğŸ‘¤ {message}</div>', unsafe_allow_html=True)
        else:
            st.markdown(f'<div class="bot-message">ğŸ¤– {message}</div>', unsafe_allow_html=True)
    
    st.markdown('</div>', unsafe_allow_html=True)
    
    # ì…ë ¥ ì˜ì—­
    st.markdown('<div class="input-container">', unsafe_allow_html=True)
    
    col1, col2 = st.columns([4, 1])
    
    with col1:
        user_input = st.text_input(
            "",
            placeholder="ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”...",
            key="user_input",
            label_visibility="collapsed"
        )
    
    with col2:
        send_button = st.button("ì „ì†¡", type="primary", use_container_width=True)
    
    st.markdown('</div>', unsafe_allow_html=True)
    
    # ë©”ì‹œì§€ ì²˜ë¦¬
    if send_button and user_input:
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        st.session_state.conversation.append(("user", user_input))
        
        # ë¬¸ì„œ ê²€ìƒ‰
        context = ""
        if st.session_state.documents and st.session_state.embeddings is not None:
            with st.spinner("ğŸ” ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘..."):
                # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
                search_results = rag_system.hybrid_search(
                    user_input,
                    st.session_state.documents,
                    st.session_state.embeddings,
                    top_k=search_k,
                    alpha=alpha
                )
                
                # Re-ranking
                relevant_docs = rag_system.rerank_documents(user_input, search_results, max_docs=3)
                context = "\n\n".join(relevant_docs)
        
        # AI ì‘ë‹µ ìƒì„±
        with st.spinner(f"ğŸ¤– {ai_model} ì‘ë‹µ ìƒì„± ì¤‘..."):
            if ai_model == "Luxia":
                response = generate_luxia_response(user_input, context, api_key)
            else:
                response = generate_gemini_response(user_input, context, st.session_state.model)
        
        # ë´‡ ì‘ë‹µ ì¶”ê°€
        st.session_state.conversation.append(("bot", response))
        
        # í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨
        st.rerun()
    
    # í•˜ë‹¨ ì •ë³´
    if st.session_state.documents:
        st.markdown("---")
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.markdown('<div class="metric-card">', unsafe_allow_html=True)
            st.metric("ğŸ“š ë¡œë“œëœ ë¬¸ì„œ", f"{len(st.session_state.documents)}ê°œ ì²­í¬")
            st.markdown('</div>', unsafe_allow_html=True)
        
        with col2:
            st.markdown('<div class="metric-card">', unsafe_allow_html=True)
            st.metric("ğŸ¤– ì‚¬ìš© ì¤‘ì¸ ëª¨ë¸", ai_model)
            st.markdown('</div>', unsafe_allow_html=True)
        
        with col3:
            st.markdown('<div class="metric-card">', unsafe_allow_html=True)
            st.metric("ğŸ’¬ ëŒ€í™” ìˆ˜", len(st.session_state.conversation) // 2)
            st.markdown('</div>', unsafe_allow_html=True)

if __name__ == "__main__":
    main()